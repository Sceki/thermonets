{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thermoNET neural differentiable model for NRLMSISE-00\n",
    "\n",
    "In this notebook, we train a neural network model to learn the NRLMSISE-00 empirical model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import thermonets as tn\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the data generated via `/scripts/generate_nrlmsise00_db.py` and print the columns\n",
    "#note that columns can be (len 16):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'wind zonal [m/s]', 'wind meridional [m/s]', 'density [kg/m^3]'\n",
    "#or (len 14):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'density [kg/m^3]'\n",
    "db=np.loadtxt('../dbs/nrlmsise00_db.txt',delimiter=',',skiprows=1)\n",
    "print(f'Shape of database is: {db.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renames some of the db content with readable names\n",
    "days=db[:,0]\n",
    "months=db[:,1]\n",
    "years=db[:,2]\n",
    "hours=db[:,3]\n",
    "minutes=db[:,4]\n",
    "seconds=db[:,5]\n",
    "microseconds=db[:,6]\n",
    "alt=db[:,7]\n",
    "# Geodetic longitude and latitude are converted in radians:\n",
    "lat=np.deg2rad(db[:,8])\n",
    "lon=np.deg2rad(db[:,9])\n",
    "# Space weather indices:\n",
    "f107a=db[:,10]\n",
    "f107=db[:,11]\n",
    "ap=db[:,12]\n",
    "# Atmospheric density as well:\n",
    "target_density=db[:,-1]\n",
    "\n",
    "# We need to extract from the db also the doy (Day of Year) and the sid (seconds in day)\n",
    "seconds_in_day=hours*3600+minutes*60+seconds+microseconds/1e6\n",
    "print('seconds in day min and max:')\n",
    "print(seconds_in_day.min(), seconds_in_day.max())\n",
    "doys=np.zeros(db.shape[0])\n",
    "for i in range(len(db)):\n",
    "    #date is a string, so I first convert it to datetime:\n",
    "    date_=datetime.datetime(year=int(years[i]), \n",
    "                            month=int(months[i]), \n",
    "                            day=int(days[i]),\n",
    "                            hour=int(hours[i]),\n",
    "                            minute=int(minutes[i]),\n",
    "                            second=int(seconds[i]),\n",
    "                            microsecond=int(microseconds[i]))\n",
    "    doys[i]=date_.timetuple().tm_yday\n",
    "print('day of the year min and max:')\n",
    "print(doys.min(), doys.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_processed=np.zeros((db.shape[0],13))\n",
    "db_processed[:,0]=np.sin(lon)\n",
    "db_processed[:,1]=np.cos(lon)\n",
    "db_processed[:,2]=tn.normalize_min_max(lat,-np.pi/2,np.pi/2)\n",
    "db_processed[:,3]=np.sin(2*np.pi*seconds_in_day/86400.)\n",
    "db_processed[:,4]=np.cos(2*np.pi*seconds_in_day/86400.)\n",
    "db_processed[:,5]=np.sin(2*np.pi*doys/365.25)\n",
    "db_processed[:,6]=np.cos(2*np.pi*doys/365.25)\n",
    "db_processed[:,7]=tn.normalize_min_max(f107, 60., 266.)\n",
    "db_processed[:,8]=tn.normalize_min_max(f107a, 60., 170.)\n",
    "db_processed[:,9]=tn.normalize_min_max(ap, 0., 110.)\n",
    "db_processed[:,10]=tn.normalize_min_max(alt, 170., 1010.)\n",
    "\n",
    "#Add the non-normalized density & altitude columns (useful to extract during training):\n",
    "db_processed[:,11]= alt\n",
    "db_processed[:,12]= target_density\n",
    "\n",
    "# Cross check that the max is <=1 and the min is >=-1\n",
    "print(f\"maximum and minimum of all the normalized data: {db_processed[:,7:11].max()}, {db_processed[:,7:11].min()}\")\n",
    "print(f\"maximum and minimum of target density: {target_density.max()}, {target_density.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = torch.tensor(db_processed, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN hyperparameters\n",
    "device = torch.device('cpu')\n",
    "minibatch_size = 512\n",
    "model_path = None #pass a path to a model in case you want to continue training from a file\n",
    "lr = 0.001\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN creation\n",
    "model = tn.ffnn(input_dim=db_processed.shape[1]-3,\n",
    "                        hidden_layer_dims=[32, 32],\n",
    "                        output_dim=12,\n",
    "                        mid_activation=torch.nn.Tanh(),\n",
    "                        last_activation=torch.nn.Tanh()).to(device)\n",
    "\n",
    "if model_path is not None:\n",
    "    model.load_state_dict(torch.load(model_path,\n",
    "                                     map_location=device.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25,50,75,100,125,150,175,200,225,230,240,250,260,270], gamma=0.8, verbose=False)\n",
    "#criterion = tn.MAPE()\n",
    "criterion = tn.MSE_LOG10()\n",
    "\n",
    "# And the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(torch_data, \n",
    "                                         batch_size=minibatch_size, \n",
    "                                         shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of model parameters: {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the global fit (see notebook: `rho_global_fit.ipynb`: this will be the baseline from which we ask the NN to learn corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../global_fits/global_fit_nrlmsise00_180.0-1000.0-4.txt','rb') as f:\n",
    "    best_global_fit=torch.from_numpy(pickle.load(f)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "ratio_losses=[]\n",
    "loss_plot = []\n",
    "mse_per_minibatch_nn=[]\n",
    "mape_per_minibatch_nn=[]\n",
    "mse_per_minibatch_fit=[]\n",
    "mape_per_minibatch_fit=[]\n",
    "best_loss_total = np.inf\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx,el in enumerate(dataloader):\n",
    "        minibatch=el[:,:-3].to(device)\n",
    "        altitude=el[:,-2].to(device)\n",
    "        rho_target=el[:,-1].to(device)\n",
    "        delta_params = model(minibatch).to(device)\n",
    "\n",
    "        #Constructs the inputs for the compute_approximated_density function as corrections from the global fit:\n",
    "        params = best_global_fit*(1+delta_params)\n",
    "        rho_nn=tn.rho_approximation(h=altitude,\n",
    "                                                params=params,\n",
    "                                                backend='torch')\n",
    "        rho_fit=tn.rho_approximation(h=altitude,\n",
    "                                             params=best_global_fit,\n",
    "                                             backend='torch')\n",
    "\n",
    "        loss = criterion(rho_nn, rho_target)\n",
    "\n",
    "        #Computes the global fit loss:\n",
    "        loss_fit =  criterion(rho_fit, rho_target)\n",
    "\n",
    "        # Zeroes the gradient \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #We compute the logged quantities\n",
    "        mse_per_minibatch_nn.append(loss.item())\n",
    "        mape_per_minibatch_nn.append(tn.mean_absolute_percentage_error(rho_nn, rho_target).item())\n",
    "        \n",
    "        #Now the same but for the global fit:\n",
    "        mse_per_minibatch_fit.append(loss_fit.item())\n",
    "        mape_per_minibatch_fit.append(tn.mean_absolute_percentage_error(rho_fit, rho_target).item())\n",
    "\n",
    "        #Ratio of the loss between the NN and the fit (the lower, the more the NN is doing better than a global fit)\n",
    "        ratio_losses.append(loss.item()/loss_fit.item())\n",
    "        \n",
    "        #Save the best model (this is wrong and should be done on the dataset):\n",
    "        if loss.item()<best_loss:    \n",
    "            best_loss=loss.item()\n",
    "\n",
    "        #Print every 10 minibatches:\n",
    "        if batch_idx%10:    \n",
    "            print(f'minibatch: {batch_idx}/{len(dataloader)}, ratio: {ratio_losses[-1]:.4e}, best minibatch loss till now: {best_loss:.4e}, loss & MAPE -----  NN: {loss.item():.10f}, {mape_per_minibatch_nn[-1]:.7f}; fit: {loss_fit.item():.10f}, {mape_per_minibatch_fit[-1]:.7f}', end='\\r')\n",
    "    \n",
    "    # We compute, at the end of the epoch and thus on the whole dataset, the losses.\n",
    "    delta_params = model(torch_data[:,:-3]).to(device)\n",
    "    params = best_global_fit*(1+delta_params)\n",
    "    rho_nn_total=tn.rho_approximation(h=torch_data[:, -2],\n",
    "                                            params=params,\n",
    "                                            backend='torch')\n",
    "\n",
    "    # First the nn loss\n",
    "    loss_total = criterion(rho_nn_total, torch_data[:, -1])\n",
    "    mape_total = tn.MAPE()(rho_nn_total, torch_data[:, -1])\n",
    "    loss_plot.append(loss_total.item())\n",
    "\n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step()\n",
    "    \n",
    "    #Print at the end of the epoch\n",
    "    curr_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    print(\" \"*300, end=\"\\r\")\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, lr: {curr_lr:.1e}, loss: {loss_total.item():.3e},  MAPE: {mape_total.item():.3f}')\n",
    "    \n",
    "    #updating torch best model:\n",
    "    if loss_total.item() < best_loss_total:\n",
    "        torch.save(model.state_dict(), f'../models/nrlmsise00_model_xxx.pyt')\n",
    "        best_loss_total=loss_total.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermonets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
