{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import thermonets as tn\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of database is: (999800, 16)\n"
     ]
    }
   ],
   "source": [
    "#I load the data generated via `/scripts/generate_nrlmsise00_db.py` and print the columns\n",
    "#note that columns can be (len 16):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'wind zonal [m/s]', 'wind meridional [m/s]', 'density [kg/m^3]'\n",
    "#or (len 14):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'density [kg/m^3]'\n",
    "db=np.loadtxt('../dbs/nrlmsise00_db.txt',delimiter=',',skiprows=1)\n",
    "print(f'Shape of database is: {db.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds in day min and max:\n",
      "0.042381 86396.718623\n",
      "day of the year min and max:\n",
      "1.0 365.0\n"
     ]
    }
   ],
   "source": [
    "#I now construct the day of the year and seconds in day:\n",
    "years=db[:,2]\n",
    "months=db[:,1]\n",
    "days=db[:,0]\n",
    "hours=db[:,3]\n",
    "minutes=db[:,4]\n",
    "seconds=db[:,5]\n",
    "microseconds=db[:,6]\n",
    "seconds_in_day=hours*3600+minutes*60+seconds+microseconds/1e6\n",
    "print('seconds in day min and max:')\n",
    "print(seconds_in_day.min(), seconds_in_day.max())\n",
    "doys=np.zeros(db.shape[0])\n",
    "for i in range(len(db)):\n",
    "    #date is a string, so I first convert it to datetime:\n",
    "    date_=datetime.datetime(year=int(years[i]), \n",
    "                            month=int(months[i]), \n",
    "                            day=int(days[i]),\n",
    "                            hour=int(hours[i]),\n",
    "                            minute=int(minutes[i]),\n",
    "                            second=int(seconds[i]),\n",
    "                            microsecond=int(microseconds[i]))\n",
    "    doys[i]=date_.timetuple().tm_yday\n",
    "print('day of the year min and max:')\n",
    "print(doys.min(), doys.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I extract the altitude:\n",
    "alt=db[:,7]\n",
    "#I now extract the longitude and latitude, and convert them to radians:\n",
    "lat=np.deg2rad(db[:,8])\n",
    "lon=np.deg2rad(db[:,9])\n",
    "#now the space weather indices:\n",
    "f107a=db[:,10]\n",
    "f107=db[:,11]\n",
    "ap=db[:,12]\n",
    "#let's extract the target density as well:\n",
    "target_density=db[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_normalized=np.zeros((db.shape[0],13))\n",
    "db_normalized[:,0]=np.sin(lon)\n",
    "db_normalized[:,1]=np.cos(lon)\n",
    "db_normalized[:,2]=np.sin(lat)\n",
    "db_normalized[:,3]=np.sin(2*np.pi*seconds_in_day/86400.)\n",
    "db_normalized[:,4]=np.cos(2*np.pi*seconds_in_day/86400.)\n",
    "db_normalized[:,5]=np.sin(2*np.pi*doys/365.25)\n",
    "db_normalized[:,6]=np.cos(2*np.pi*doys/365.25)\n",
    "db_normalized[:,7]=tn.normalize_min_max(alt, 150., 650.)\n",
    "db_normalized[:,8]=tn.normalize_min_max(f107, 60., 290.)\n",
    "db_normalized[:,9]=tn.normalize_min_max(f107a, 50., 190.)\n",
    "db_normalized[:,10]=tn.normalize_min_max(ap, 0., 140.)\n",
    "#I add the non-normalized density & altitude columns (useful to extract during training):\n",
    "db_normalized[:,11]= alt\n",
    "db_normalized[:,12]= target_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum and minimum of all the normalized data: 1.0, -1.0\n",
      "maximum and minimum of target density: 1.712652587471857e-09, 1.9853483043363358e-15\n"
     ]
    }
   ],
   "source": [
    "#cross check that the max is <=1 and the min is >=-1\n",
    "print(f\"maximum and minimum of all the normalized data: {db_normalized[:,:11].max()}, {db_normalized[:,:11].min()}\")\n",
    "print(f\"maximum and minimum of target density: {target_density.max()}, {target_density.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = torch.tensor(db_normalized,\n",
    "                          dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN hyperparameters\n",
    "device = torch.device('cpu')\n",
    "batch_size = 4096\n",
    "model_path = None #pass a path to a model in case you want to continue training from a file\n",
    "lr = 0.00001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader creation\n",
    "dataloader = torch.utils.data.DataLoader(torch_data, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN creation\n",
    "model = tn.ffnn(input_dim=db_normalized.shape[1]-2,\n",
    "                        hidden_layer_dims=[32, 32],\n",
    "                        output_dim=12,\n",
    "                        mid_activation=torch.nn.Tanh(),\n",
    "                        last_activation=torch.nn.Tanh()).to(device)\n",
    "\n",
    "if model_path is not None:\n",
    "    model.load_state_dict(torch.load(model_path,\n",
    "                                     map_location=device.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters: 1836\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of model parameters: {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the global fit (see notebook: `rho_global_fit.ipynb`: this will be the baseline from which we ask the NN to learn corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../global_fits/global_fit_nrlmsise00_180.0-1000.0-4.txt','rb') as f:\n",
    "    best_global_fit=torch.from_numpy(pickle.load(f)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1/100, average RMSE (log10) loss: 0.24944231060086464, average MAPE: 67.14630390089386, ----  NN: 0.2057335377, 59.3295326; fit: 0.0732013658, 53.8712158\n",
      "End of epoch 2/100, average RMSE (log10) loss: 0.16142051974121405, average MAPE: 62.56833642453564, ----  NN: 0.1398187578, 62.8487778; fit: 0.0752398595, 60.1677132\n",
      "End of epoch 3/100, average RMSE (log10) loss: 0.11007382480465636, average MAPE: 57.672035108293805, ---  NN: 0.0852707401, 51.8967285; fit: 0.0626801699, 53.4257774\n",
      "End of epoch 4/100, average RMSE (log10) loss: 0.07967869681971414, average MAPE: 52.274841464295676, ---  NN: 0.0666145235, 50.1263390; fit: 0.0787915364, 64.1585922\n",
      "End of epoch 5/100, average RMSE (log10) loss: 0.06082096440451486, average MAPE: 46.652806464993226, ---  NN: 0.0606885962, 47.8941994; fit: 0.0831792653, 60.4612007\n",
      "End of epoch 6/100, average RMSE (log10) loss: 0.04841321878591362, average MAPE: 41.44194877780214, ----  NN: 0.0461282916, 38.7175751; fit: 0.0808902904, 59.9642639\n",
      "End of epoch 7/100, average RMSE (log10) loss: 0.04001176748044637, average MAPE: 37.266537662428256, ---  NN: 0.0377397276, 36.2449989; fit: 0.0767223611, 62.3776970\n",
      "End of epoch 8/100, average RMSE (log10) loss: 0.034266893725310055, average MAPE: 34.21693378370635, ---  NN: 0.0330245681, 33.7078857; fit: 0.0668599084, 54.2577934\n",
      "End of epoch 9/100, average RMSE (log10) loss: 0.030332895467171862, average MAPE: 32.08143973837093, ---  NN: 0.0276887957, 30.8026543; fit: 0.0707809553, 57.0820923\n",
      "End of epoch 10/100, average RMSE (log10) loss: 0.02766154851688414, average MAPE: 30.62906426410286, ---  NN: 0.0242710318, 27.5993195; fit: 0.0733887106, 56.7592697\n",
      "End of epoch 11/100, average RMSE (log10) loss: 0.02586143535314774, average MAPE: 29.628092785270848, --  NN: 0.0245003644, 29.2758732; fit: 0.0753092691, 59.8034439\n",
      "End of epoch 12/100, average RMSE (log10) loss: 0.024639307023311147, average MAPE: 28.91515926438935, --  NN: 0.0264169388, 31.1004105; fit: 0.0726254061, 56.3969536\n",
      "End of epoch 13/100, average RMSE (log10) loss: 0.02375533188484153, average MAPE: 28.34331057023029, ---  NN: 0.0209914986, 28.4432507; fit: 0.0708207190, 59.5495148\n",
      "End of epoch 14/100, average RMSE (log10) loss: 0.023131527994968454, average MAPE: 27.905334083401428, -  NN: 0.0266652033, 30.3781548; fit: 0.0831547081, 66.3586349\n",
      "End of epoch 15/100, average RMSE (log10) loss: 0.022587149727101227, average MAPE: 27.499175636135803, -  NN: 0.0211379398, 26.7987766; fit: 0.0762654543, 61.1493645\n",
      "End of epoch 16/100, average RMSE (log10) loss: 0.022130274050393882, average MAPE: 27.139563299685108, -  NN: 0.0224837791, 27.4133091; fit: 0.0740432069, 58.1904411\n",
      "End of epoch 17/100, average RMSE (log10) loss: 0.02168513815469888, average MAPE: 26.785787053010903, --  NN: 0.0191152357, 25.5071411; fit: 0.0837706774, 67.5275192\n",
      "End of epoch 18/100, average RMSE (log10) loss: 0.02126615168336703, average MAPE: 26.4494921392324, ----  NN: 0.0199096706, 26.2823009; fit: 0.0690617636, 55.4216461\n",
      "End of epoch 19/100, average RMSE (log10) loss: 0.020848208939542576, average MAPE: 26.118937192644392, -  NN: 0.0199354049, 25.9141026; fit: 0.0742425844, 58.2533951\n",
      "End of epoch 20/100, average RMSE (log10) loss: 0.020426561447734734, average MAPE: 25.78645861878687, --  NN: 0.0194554869, 25.3739948; fit: 0.0784707144, 61.9030304\n",
      "End of epoch 21/100, average RMSE (log10) loss: 0.01999504866496641, average MAPE: 25.44554841956314, ---  NN: 0.0171140786, 24.1218643; fit: 0.0644305274, 53.8525391\n",
      "End of epoch 22/100, average RMSE (log10) loss: 0.019576664466638954, average MAPE: 25.127335497797752, -  NN: 0.0196252130, 25.4044800; fit: 0.0781239346, 57.6434898\n",
      "End of epoch 23/100, average RMSE (log10) loss: 0.01914056398886807, average MAPE: 24.78454688714475, ---  NN: 0.0186225437, 23.9779968; fit: 0.0771007165, 57.3549156\n",
      "End of epoch 24/100, average RMSE (log10) loss: 0.01870017440191337, average MAPE: 24.459168585952447, --  NN: 0.0174592808, 23.3498745; fit: 0.0743760988, 55.2045364\n",
      "End of epoch 25/100, average RMSE (log10) loss: 0.018259912349131643, average MAPE: 24.121753404578385, -  NN: 0.0174198933, 24.6769695; fit: 0.0744580328, 57.2432404\n",
      "End of epoch 26/100, average RMSE (log10) loss: 0.017814524312104497, average MAPE: 23.77895942999392, --  NN: 0.0164090693, 21.6580601; fit: 0.0714778081, 53.0934219\n",
      "End of epoch 27/100, average RMSE (log10) loss: 0.017373252257096523, average MAPE: 23.459442068606005, -  NN: 0.0160291158, 22.8197212; fit: 0.0815897137, 63.0883560\n",
      "End of epoch 28/100, average RMSE (log10) loss: 0.016934334015359685, average MAPE: 23.11513970822704, --  NN: 0.0154659674, 20.9313774; fit: 0.0728021935, 56.6429482\n",
      "End of epoch 29/100, average RMSE (log10) loss: 0.016491061775964135, average MAPE: 22.781994512129803, -  NN: 0.0130397808, 20.4253368; fit: 0.0668352693, 54.5955734\n",
      "End of epoch 30/100, average RMSE (log10) loss: 0.01608394144140944, average MAPE: 22.46514637226961, ---  NN: 0.0192977581, 24.5709362; fit: 0.0767665803, 57.8906479\n",
      "End of epoch 31/100, average RMSE (log10) loss: 0.015647154141749654, average MAPE: 22.12199040626993, --  NN: 0.0167490114, 22.8311977; fit: 0.0783052742, 56.9853363\n",
      "End of epoch 32/100, average RMSE (log10) loss: 0.015222108253867042, average MAPE: 21.785772027774733, -  NN: 0.0166419111, 22.4368095; fit: 0.0759999678, 58.9123116\n",
      "End of epoch 33/100, average RMSE (log10) loss: 0.014794683711109112, average MAPE: 21.440735859773596, -  NN: 0.0149135636, 21.6048470; fit: 0.0741507336, 61.4081879\n",
      "End of epoch 34/100, average RMSE (log10) loss: 0.014373592378533617, average MAPE: 21.092429211674904, -  NN: 0.0134337265, 20.9698582; fit: 0.0775600821, 60.3190422\n",
      "End of epoch 35/100, average RMSE (log10) loss: 0.013956127171309627, average MAPE: 20.744123809191645, -  NN: 0.0120459404, 19.0123596; fit: 0.0685075745, 54.2371941\n",
      "End of epoch 36/100, average RMSE (log10) loss: 0.0135435872312103, average MAPE: 20.396599150677115, ---  NN: 0.0104562845, 17.9389172; fit: 0.0684524849, 54.3587036\n",
      "End of epoch 37/100, average RMSE (log10) loss: 0.01314636171244237, average MAPE: 20.062317167009628, --  NN: 0.0115843415, 18.4008427; fit: 0.0805147216, 65.2467651\n",
      "End of epoch 38/100, average RMSE (log10) loss: 0.012756361112910874, average MAPE: 19.728348829308334, -  NN: 0.0130372709, 20.8186245; fit: 0.0776787326, 60.7182159\n",
      "End of epoch 39/100, average RMSE (log10) loss: 0.012358802861096908, average MAPE: 19.388727265961315, -  NN: 0.0111431452, 17.5986347; fit: 0.0847997442, 69.3238525\n",
      "End of epoch 40/100, average RMSE (log10) loss: 0.011977381837003085, average MAPE: 19.063239249404596, -  NN: 0.0122777792, 18.6097355; fit: 0.0762947276, 60.2896881\n",
      "End of epoch 41/100, average RMSE (log10) loss: 0.011591787282757613, average MAPE: 18.741339259244956, -  NN: 0.0125355832, 20.4832458; fit: 0.0799334198, 61.1328468\n",
      "End of epoch 42/100, average RMSE (log10) loss: 0.011186278394746537, average MAPE: 18.39798676627023, --  NN: 0.0099571589, 17.6588840; fit: 0.0803504884, 63.2296028\n",
      "End of epoch 43/100, average RMSE (log10) loss: 0.010757476718602131, average MAPE: 18.04001854098573, --  NN: 0.0083409436, 16.1669731; fit: 0.0720880404, 58.8845863\n",
      "End of epoch 44/100, average RMSE (log10) loss: 0.01027969157285228, average MAPE: 17.64616488242636, ---  NN: 0.0086698290, 16.2793732; fit: 0.0597880743, 50.3205910\n",
      "End of epoch 45/100, average RMSE (log10) loss: 0.009738699259350494, average MAPE: 17.194021614230408, -  NN: 0.0102170501, 17.1143150; fit: 0.0837190300, 61.0899429\n",
      "End of epoch 46/100, average RMSE (log10) loss: 0.009181602441762783, average MAPE: 16.70340747443997, --  NN: 0.0076828511, 15.6324310; fit: 0.0763666183, 63.1878510\n",
      "End of epoch 47/100, average RMSE (log10) loss: 0.008735223954581485, average MAPE: 16.260625123004523, -  NN: 0.0068337759, 14.3136902; fit: 0.0716911703, 57.6927910\n",
      "End of epoch 48/100, average RMSE (log10) loss: 0.008415455920431686, average MAPE: 15.93584589276995, --  NN: 0.0077655301, 15.7274485; fit: 0.0725299492, 55.1601105\n",
      "End of epoch 49/100, average RMSE (log10) loss: 0.008166481003317298, average MAPE: 15.665368259196379, -  NN: 0.0075166211, 15.6739311; fit: 0.0585259087, 50.5731812\n",
      "End of epoch 50/100, average RMSE (log10) loss: 0.007963402742253883, average MAPE: 15.442693605228346, -  NN: 0.0080739716, 15.3540716; fit: 0.0733641684, 56.0840302\n",
      "End of epoch 51/100, average RMSE (log10) loss: 0.007781592357371535, average MAPE: 15.242397674249142, -  NN: 0.0080414275, 14.4129362; fit: 0.0728977397, 58.8790054\n",
      "End of epoch 52/100, average RMSE (log10) loss: 0.007613109027472685, average MAPE: 15.061448879631198, -  NN: 0.0071198889, 14.3891964; fit: 0.0763708130, 58.7159386\n",
      "End of epoch 53/100, average RMSE (log10) loss: 0.007457770536444625, average MAPE: 14.889069549404844, -  NN: 0.0069135125, 14.0223236; fit: 0.0693443194, 53.2026749\n",
      "End of epoch 54/100, average RMSE (log10) loss: 0.0073174206463962185, average MAPE: 14.7350789400996, --  NN: 0.0082935728, 15.8815947; fit: 0.0674708709, 57.2313843\n",
      "End of epoch 55/100, average RMSE (log10) loss: 0.007176181751930592, average MAPE: 14.575784317328006, -  NN: 0.0075336192, 15.2999697; fit: 0.0848474726, 63.4854240\n",
      "End of epoch 56/100, average RMSE (log10) loss: 0.007043096539089266, average MAPE: 14.423576281022052, -  NN: 0.0073166555, 14.2261372; fit: 0.0888550505, 67.0018463\n",
      "End of epoch 57/100, average RMSE (log10) loss: 0.006912252012336133, average MAPE: 14.27515312506228, --  NN: 0.0061512794, 13.0865688; fit: 0.0603304058, 48.8422699\n",
      "End of epoch 58/100, average RMSE (log10) loss: 0.006795987500143903, average MAPE: 14.137511623149015, -  NN: 0.0076083071, 14.4428101; fit: 0.0744996816, 53.6280975\n",
      "End of epoch 59/100, average RMSE (log10) loss: 0.006670948247216185, average MAPE: 13.999681807537469, -  NN: 0.0056596077, 13.9238272; fit: 0.0689809546, 56.1429901\n",
      "End of epoch 60/100, average RMSE (log10) loss: 0.006562209674822433, average MAPE: 13.862732369559152, -  NN: 0.0071286527, 14.3499088; fit: 0.0676902011, 55.7293816\n",
      "End of epoch 61/100, average RMSE (log10) loss: 0.00644476908658232, average MAPE: 13.727512706055933, --  NN: 0.0052941428, 12.6759987; fit: 0.0616858676, 50.3679390\n",
      "End of epoch 62/100, average RMSE (log10) loss: 0.006343062305632903, average MAPE: 13.606415904298121, -  NN: 0.0064345738, 13.9485464; fit: 0.0819389746, 65.1498489\n",
      "End of epoch 63/100, average RMSE (log10) loss: 0.006240956129848349, average MAPE: 13.475266168555434, -  NN: 0.0068780389, 13.4919691; fit: 0.0867414698, 64.8421555\n",
      "End of epoch 64/100, average RMSE (log10) loss: 0.0061430045346520384, average MAPE: 13.354401389919982,   NN: 0.0073247482, 14.0936165; fit: 0.0761441141, 56.3408356\n",
      "End of epoch 65/100, average RMSE (log10) loss: 0.006040975666243811, average MAPE: 13.231552809111927, -  NN: 0.0055151791, 12.7001362; fit: 0.0718198195, 59.0406609\n",
      "End of epoch 66/100, average RMSE (log10) loss: 0.005948656537018869, average MAPE: 13.114326052763024, -  NN: 0.0054066177, 12.7661819; fit: 0.0660446957, 51.5031586\n",
      "End of epoch 67/100, average RMSE (log10) loss: 0.00586043157595761, average MAPE: 13.004527913307657, --  NN: 0.0055481140, 13.2705479; fit: 0.0743330568, 56.1703911\n",
      "End of epoch 68/100, average RMSE (log10) loss: 0.00577540394222858, average MAPE: 12.892166472454461, --  NN: 0.0055508157, 12.3262796; fit: 0.0806941837, 59.1662712\n",
      "End of epoch 69/100, average RMSE (log10) loss: 0.005692261181847781, average MAPE: 12.784536494040976, -  NN: 0.0051146266, 12.1774216; fit: 0.0722822621, 55.2842979\n",
      "End of epoch 70/100, average RMSE (log10) loss: 0.005616970044769803, average MAPE: 12.685748547923808, -  NN: 0.0058494844, 12.9183846; fit: 0.0752335563, 57.6640015\n",
      "End of epoch 71/100, average RMSE (log10) loss: 0.005541696395648985, average MAPE: 12.591993518751496, -  NN: 0.0054026651, 12.9878492; fit: 0.0750580207, 58.7367706\n",
      "End of epoch 72/100, average RMSE (log10) loss: 0.00547249244373976, average MAPE: 12.494558190326302, --  NN: 0.0057042590, 12.7572193; fit: 0.0817392766, 62.5335579\n",
      "End of epoch 73/100, average RMSE (log10) loss: 0.005402589403092861, average MAPE: 12.40456061071279, --  NN: 0.0048442869, 11.9115372; fit: 0.0703187585, 55.7317657\n",
      "End of epoch 74/100, average RMSE (log10) loss: 0.005339823552996528, average MAPE: 12.320023727416991, -  NN: 0.0050016823, 11.7941961; fit: 0.0768023059, 59.2385063\n",
      "End of epoch 75/100, average RMSE (log10) loss: 0.005282097437171912, average MAPE: 12.241586996584523, -  NN: 0.0057786456, 12.8320961; fit: 0.0877891630, 61.4701271\n",
      "End of epoch 76/100, average RMSE (log10) loss: 0.005227040989818622, average MAPE: 12.168884133319466, -  NN: 0.0064026928, 13.7362633; fit: 0.0706750825, 57.0378075\n",
      "End of epoch 77/100, average RMSE (log10) loss: 0.005166641867966676, average MAPE: 12.08679480649987, --  NN: 0.0048472905, 12.0986347; fit: 0.0634941831, 50.3434944\n",
      "End of epoch 78/100, average RMSE (log10) loss: 0.00511763180435008, average MAPE: 12.020329125073491, --  NN: 0.0055729831, 12.4855633; fit: 0.0756662264, 61.8241806\n",
      "End of epoch 79/100, average RMSE (log10) loss: 0.005067393556237221, average MAPE: 11.950762628049267, -  NN: 0.0052458369, 12.2099352; fit: 0.0893871859, 67.4264374\n",
      "End of epoch 80/100, average RMSE (log10) loss: 0.005019264616908468, average MAPE: 11.886418595605967, -  NN: 0.0048425370, 10.9470959; fit: 0.0691914037, 56.4564095\n",
      "End of epoch 81/100, average RMSE (log10) loss: 0.004977430545781948, average MAPE: 11.831504163936692, -  NN: 0.0054254457, 12.4320698; fit: 0.0808861628, 64.0189438\n",
      "End of epoch 82/100, average RMSE (log10) loss: 0.004929752274397381, average MAPE: 11.765156255449568, -  NN: 0.0038803529, 10.4567375; fit: 0.0742690340, 60.3286819\n",
      "End of epoch 83/100, average RMSE (log10) loss: 0.004895734404954983, average MAPE: 11.719577119788346, -  NN: 0.0054665525, 12.3459053; fit: 0.0737935156, 53.0951157\n",
      "End of epoch 84/100, average RMSE (log10) loss: 0.004854351681257997, average MAPE: 11.662941068532515, -  NN: 0.0046721627, 11.6344290; fit: 0.0817056745, 64.8966675\n",
      "End of epoch 85/100, average RMSE (log10) loss: 0.0048150281497866525, average MAPE: 11.608724457877022,   NN: 0.0039290073, 10.5346193; fit: 0.0686904192, 57.2310333\n",
      "End of epoch 86/100, average RMSE (log10) loss: 0.004781343784107238, average MAPE: 11.563621921928561, -  NN: 0.0042754202, 11.0207577; fit: 0.0732387453, 57.2880859\n",
      "End of epoch 87/100, average RMSE (log10) loss: 0.004751079060061246, average MAPE: 11.51690029611393, --  NN: 0.0052290773, 10.9149323; fit: 0.0726326779, 63.2924728\n",
      "End of epoch 88/100, average RMSE (log10) loss: 0.004720430167353883, average MAPE: 11.47613536679015, --  NN: 0.0057238513, 11.7877197; fit: 0.0826490670, 56.2827110\n",
      "End of epoch 89/100, average RMSE (log10) loss: 0.004687697090664689, average MAPE: 11.432278539696519, -  NN: 0.0052939183, 11.6171980; fit: 0.0704516172, 53.8694077\n",
      "End of epoch 90/100, average RMSE (log10) loss: 0.004656406225902693, average MAPE: 11.393939909643056, -  NN: 0.0048269355, 12.1877213; fit: 0.0784823000, 58.1139870\n",
      "End of epoch 91/100, average RMSE (log10) loss: 0.004627994417536015, average MAPE: 11.352032976734394, -  NN: 0.0051691462, 12.0865984; fit: 0.0756991655, 59.7264061\n",
      "End of epoch 92/100, average RMSE (log10) loss: 0.004598196938025708, average MAPE: 11.31200894725566, --  NN: 0.0046023484, 11.8748951; fit: 0.0760237798, 57.9301224\n",
      "End of epoch 93/100, average RMSE (log10) loss: 0.0045709018652536435, average MAPE: 11.270005767199457,   NN: 0.0046969405, 10.6574669; fit: 0.0805131644, 63.4531898\n",
      "End of epoch 94/100, average RMSE (log10) loss: 0.00454240045985397, average MAPE: 11.233153744133151, --  NN: 0.0041716225, 11.0661964; fit: 0.0789706334, 55.6693192\n",
      "End of epoch 95/100, average RMSE (log10) loss: 0.004520072770894182, average MAPE: 11.202337957888233, -  NN: 0.0052108187, 12.2803011; fit: 0.0783300027, 62.6410065\n",
      "End of epoch 96/100, average RMSE (log10) loss: 0.004493078110473497, average MAPE: 11.162373694595026, -  NN: 0.0048133391, 11.1470203; fit: 0.0794731006, 61.2857399\n",
      "End of epoch 97/100, average RMSE (log10) loss: 0.004464850629850918, average MAPE: 11.124227539373903, -  NN: 0.0039092028, 10.8743439; fit: 0.0727317631, 53.2322350\n",
      "End of epoch 98/100, average RMSE (log10) loss: 0.004443472891817896, average MAPE: 11.095173454284668, -  NN: 0.0046461080, 11.6428289; fit: 0.0769501328, 59.9445534\n",
      "End of epoch 99/100, average RMSE (log10) loss: 0.004420045227268521, average MAPE: 11.060199422252422, -  NN: 0.0047316677, 11.1383162; fit: 0.0831347331, 67.5265503\n",
      "End of epoch 100/100, average RMSE (log10) loss: 0.0043944256836358385, average MAPE: 11.023571948615873,  NN: 0.0040810201, 10.9319687; fit: 0.0830670670, 64.0319138\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "ratio_losses=[]\n",
    "rmse_per_minibatch_nn=[]\n",
    "mape_per_minibatch_nn=[]\n",
    "rmse_per_minibatch_fit=[]\n",
    "mape_per_minibatch_fit=[]\n",
    "for epoch in range(epochs):\n",
    "    #model.train(True)  # Set model to training mode\n",
    "    total_rmse = 0.0\n",
    "    total_mape = 0.0\n",
    "    for batch_idx,el in enumerate(dataloader):\n",
    "        minibatch=el[:,:-2].to(device)\n",
    "        altitude=el[:,-2].to(device)\n",
    "        rho_target=el[:,-1].to(device)\n",
    "        delta_params = model(minibatch).to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear accumulated gradients\n",
    "        #now I construct the inputs for the compute_approximated_density function as corrections from the global fit:\n",
    "        params = best_global_fit*(1+delta_params)\n",
    "        rho_nn=tn.rho_approximation(h=altitude,\n",
    "                                                params=params,\n",
    "                                                backend='torch')\n",
    "        rho_fit=tn.rho_approximation(h=altitude,\n",
    "                                             params=best_global_fit,\n",
    "                                             backend='torch')\n",
    "\n",
    "        loss = criterion(torch.log10(rho_nn), torch.log10(rho_target))\n",
    "        loss.backward()\n",
    "        #I also compute the global fit loss:\n",
    "        loss_fit =  torch.nn.MSELoss()(torch.log10(rho_fit).squeeze(), torch.log10(rho_target).squeeze())\n",
    "        #I update the weights:\n",
    "        optimizer.step()\n",
    "        #let's store the losses for the NN:\n",
    "        rmse_per_minibatch_nn.append(loss.item())\n",
    "        mape_per_minibatch_nn.append(tn.mean_absolute_percentage_error(rho_nn, rho_target).item())\n",
    "        total_rmse+=rmse_per_minibatch_nn[-1]\n",
    "        total_mape+=mape_per_minibatch_nn[-1]\n",
    "        #now the same but for the global fit:\n",
    "        rmse_per_minibatch_fit.append(loss_fit.item())\n",
    "        mape_per_minibatch_fit.append(tn.mean_absolute_percentage_error(rho_fit, rho_target).item())\n",
    "\n",
    "        #ratio of the loss between the NN and the fit (the lower, the more the NN is doing better than a global fit)\n",
    "        ratio_losses.append(loss.item()/loss_fit.item())\n",
    "        #I only save the best model:\n",
    "        if batch_idx>1:\n",
    "            if rmse_per_minibatch_nn[-1]<min(rmse_per_minibatch_nn[:-1]):    \n",
    "                #updating torch best model:\n",
    "                torch.save(model.state_dict(), f'best_model.pyt')\n",
    "                best_loss=loss.item()\n",
    "                #print(f'Saving model - current best loss: {best_loss}\\n')\n",
    "        else:\n",
    "            best_loss=loss.item()\n",
    "        #I print every 10 minibatches:\n",
    "        if batch_idx%10:    \n",
    "            print(f'minibatch: {batch_idx}/{len(dataloader)}, ratio: {ratio_losses[-1]:.10f}, best loss till now: {best_loss:.10f}, loss RMSE (log10) & MAPE -----  NN: {loss.item():.10f}, {mape_per_minibatch_nn[-1]:.7f}; fit: {loss_fit.item():.10f}, {mape_per_minibatch_fit[-1]:.7f}', end='\\r')\n",
    "    #I also print at the end of the epoch\n",
    "    print(f'End of epoch {epoch + 1}/{epochs}, average RMSE (log10) loss: {total_rmse / len(dataloader)}, average MAPE: {total_mape / len(dataloader)}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermonets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
