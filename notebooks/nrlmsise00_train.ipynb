{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import thermonets as tn\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of database is: (999700, 14)\n"
     ]
    }
   ],
   "source": [
    "#Loads the data generated via `/scripts/generate_nrlmsise00_db.py` and print the columns\n",
    "#note that columns can be (len 16):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'wind zonal [m/s]', 'wind meridional [m/s]', 'density [kg/m^3]'\n",
    "#or (len 14):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'density [kg/m^3]'\n",
    "db=np.loadtxt('../dbs/nrlmsise00_db.txt',delimiter=',',skiprows=1)\n",
    "print(f'Shape of database is: {db.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds in day min and max:\n",
      "20.64977 86392.51835\n",
      "day of the year min and max:\n",
      "1.0 365.0\n"
     ]
    }
   ],
   "source": [
    "# Renames some of the db content with readable names\n",
    "days=db[:,0]\n",
    "months=db[:,1]\n",
    "years=db[:,2]\n",
    "hours=db[:,3]\n",
    "minutes=db[:,4]\n",
    "seconds=db[:,5]\n",
    "microseconds=db[:,6]\n",
    "alt=db[:,7]\n",
    "# Geodetic longitude and latitude are converted in radians:\n",
    "lat=np.deg2rad(db[:,8])\n",
    "lon=np.deg2rad(db[:,9])\n",
    "# Space weather indices:\n",
    "f107a=db[:,10]\n",
    "f107=db[:,11]\n",
    "ap=db[:,12]\n",
    "# Atmospheric density as well:\n",
    "target_density=db[:,-1]\n",
    "\n",
    "# We need to extract from the db also the doy (Day of Year) and the sid (seconds in day)\n",
    "seconds_in_day=hours*3600+minutes*60+seconds+microseconds/1e6\n",
    "print('seconds in day min and max:')\n",
    "print(seconds_in_day.min(), seconds_in_day.max())\n",
    "doys=np.zeros(db.shape[0])\n",
    "for i in range(len(db)):\n",
    "    #date is a string, so I first convert it to datetime:\n",
    "    date_=datetime.datetime(year=int(years[i]), \n",
    "                            month=int(months[i]), \n",
    "                            day=int(days[i]),\n",
    "                            hour=int(hours[i]),\n",
    "                            minute=int(minutes[i]),\n",
    "                            second=int(seconds[i]),\n",
    "                            microsecond=int(microseconds[i]))\n",
    "    doys[i]=date_.timetuple().tm_yday\n",
    "print('day of the year min and max:')\n",
    "print(doys.min(), doys.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum and minimum of all the normalized data: 0.9761904761904763, -1.0\n",
      "maximum and minimum of target density: 7.20569529348575e-10, 1.9693989513886182e-16\n"
     ]
    }
   ],
   "source": [
    "db_processed=np.zeros((db.shape[0],13))\n",
    "db_processed[:,0]=np.sin(lon)\n",
    "db_processed[:,1]=np.cos(lon)\n",
    "db_processed[:,2]=np.sin(lat)\n",
    "db_processed[:,3]=np.sin(2*np.pi*seconds_in_day/86400.)\n",
    "db_processed[:,4]=np.cos(2*np.pi*seconds_in_day/86400.)\n",
    "db_processed[:,5]=np.sin(2*np.pi*doys/365.25)\n",
    "db_processed[:,6]=np.cos(2*np.pi*doys/365.25)\n",
    "db_processed[:,7]=tn.normalize_min_max(f107, 60., 266.)\n",
    "db_processed[:,8]=tn.normalize_min_max(f107a, 60., 266.)\n",
    "db_processed[:,9]=tn.normalize_min_max(ap, 0., 110.)\n",
    "db_processed[:,10]=tn.normalize_min_max(alt, 170., 1010.)\n",
    "\n",
    "#Add the non-normalized density & altitude columns (useful to extract during training):\n",
    "db_processed[:,11]= alt\n",
    "db_processed[:,12]= target_density\n",
    "\n",
    "# Cross check that the max is <=1 and the min is >=-1\n",
    "print(f\"maximum and minimum of all the normalized data: {db_processed[:,7:11].max()}, {db_processed[:,7:11].min()}\")\n",
    "print(f\"maximum and minimum of target density: {target_density.max()}, {target_density.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = torch.tensor(db_processed, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN hyperparameters\n",
    "device = torch.device('cpu')\n",
    "minibatch_size = 4096\n",
    "model_path = None #pass a path to a model in case you want to continue training from a file\n",
    "lr = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN creation\n",
    "model = tn.ffnn(input_dim=db_processed.shape[1]-3,\n",
    "                        hidden_layer_dims=[32, 32],\n",
    "                        output_dim=12,\n",
    "                        mid_activation=torch.nn.Tanh(),\n",
    "                        last_activation=torch.nn.Tanh()).to(device)\n",
    "\n",
    "if model_path is not None:\n",
    "    model.load_state_dict(torch.load(model_path,\n",
    "                                     map_location=device.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the optimizer\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 1000, min_lr = 1e-7, verbose=False)\n",
    "#criterion = tn.MAPE()\n",
    "criterion = tn.MSE_LOG10()\n",
    "\n",
    "# And the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(torch_data, \n",
    "                                         batch_size=minibatch_size, \n",
    "                                         shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters: 1804\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of model parameters: {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the global fit (see notebook: `rho_global_fit.ipynb`: this will be the baseline from which we ask the NN to learn corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../global_fits/global_fit_nrlmsise00_180.0-1000.0-4.txt','rb') as f:\n",
    "    best_global_fit=torch.from_numpy(pickle.load(f)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, lr: 1.0e-03, average MSE (log10) loss: 0.03530084914622866, average MAPE: 34.315635307467716                                                                                                                                                             \n",
      "Epoch 2/100, lr: 1.0e-03, average MSE (log10) loss: 0.009809552684274254, average MAPE: 16.881024461862992                                                                                                                                                             \n",
      "Epoch 3/100, lr: 1.0e-03, average MSE (log10) loss: 0.00598497655128642, average MAPE: 13.301208204152633                                                                                                                                                             \n",
      "Epoch 4/100, lr: 1.0e-03, average MSE (log10) loss: 0.005012859789920705, average MAPE: 12.159194681595784                                                                                                                                                             \n",
      "Epoch 5/100, lr: 1.0e-03, average MSE (log10) loss: 0.0044622799894791475, average MAPE: 11.449686404636928                                                                                                                                                             \n",
      "Epoch 6/100, lr: 1.0e-03, average MSE (log10) loss: 0.004066567564839307, average MAPE: 10.90363613440066                                                                                                                                                             \n",
      "Epoch 7/100, lr: 1.0e-03, average MSE (log10) loss: 0.003753645491919347, average MAPE: 10.445977491262008                                                                                                                                                             \n",
      "Epoch 8/100, lr: 1.0e-03, average MSE (log10) loss: 0.0034627880131331635, average MAPE: 9.996190429220395                                                                                                                                                             \n",
      "Epoch 9/100, lr: 1.0e-03, average MSE (log10) loss: 0.003230231315163629, average MAPE: 9.629966350477568                                                                                                                                                             \n",
      "Epoch 10/100, lr: 1.0e-03, average MSE (log10) loss: 0.003078076835455639, average MAPE: 9.390854193239797                                                                                                                                                             \n",
      "Epoch 11/100, lr: 1.0e-03, average MSE (log10) loss: 0.0029144629503467254, average MAPE: 9.126863401763293                                                                                                                                                             \n",
      "Epoch 12/100, lr: 1.0e-03, average MSE (log10) loss: 0.0027759669092008656, average MAPE: 8.899382240918218                                                                                                                                                             \n",
      "Epoch 13/100, lr: 1.0e-03, average MSE (log10) loss: 0.0026417614645039547, average MAPE: 8.683802014954235                                                                                                                                                             \n",
      "Epoch 14/100, lr: 1.0e-03, average MSE (log10) loss: 0.0025168215114699335, average MAPE: 8.478409358433314                                                                                                                                                             \n",
      "Epoch 15/100, lr: 1.0e-03, average MSE (log10) loss: 0.00240672908875407, average MAPE: 8.297057005823875                                                                                                                                                             \n",
      "Epoch 16/100, lr: 1.0e-03, average MSE (log10) loss: 0.0023001134533397094, average MAPE: 8.118988982998594                                                                                                                                                             \n",
      "Epoch 17/100, lr: 1.0e-03, average MSE (log10) loss: 0.0022127483999926827, average MAPE: 7.9673534763102625                                                                                                                                                             \n",
      "Epoch 18/100, lr: 1.0e-03, average MSE (log10) loss: 0.0021357613881783826, average MAPE: 7.837192648284289                                                                                                                                                             \n",
      "Epoch 19/100, lr: 1.0e-03, average MSE (log10) loss: 0.00204631499280887, average MAPE: 7.669340729226872                                                                                                                                                             \n",
      "Epoch 20/100, lr: 1.0e-03, average MSE (log10) loss: 0.0019941335049818974, average MAPE: 7.578098199805435                                                                                                                                                             \n",
      "Epoch 21/100, lr: 1.0e-03, average MSE (log10) loss: 0.0019418784968402922, average MAPE: 7.48382706934092                                                                                                                                                             \n",
      "Epoch 22/100, lr: 1.0e-03, average MSE (log10) loss: 0.001880453867191563, average MAPE: 7.362595404410849                                                                                                                                                             \n",
      "Epoch 23/100, lr: 8.0e-04, average MSE (log10) loss: 0.0018321533758687426, average MAPE: 7.26614871511654                                                                                                                                                             \n",
      "Epoch 24/100, lr: 8.0e-04, average MSE (log10) loss: 0.0017914378069987407, average MAPE: 7.1856937836627575                                                                                                                                                             \n",
      "Epoch 25/100, lr: 8.0e-04, average MSE (log10) loss: 0.0017679456355316299, average MAPE: 7.14320663335372                                                                                                                                                             \n",
      "Epoch 26/100, lr: 8.0e-04, average MSE (log10) loss: 0.0017335920300030586, average MAPE: 7.070168218807298                                                                                                                                                             \n",
      "Epoch 27/100, lr: 6.4e-04, average MSE (log10) loss: 0.0016890277435090773, average MAPE: 6.97575207340474                                                                                                                                                             \n",
      "Epoch 28/100, lr: 6.4e-04, average MSE (log10) loss: 0.0016588457351626486, average MAPE: 6.912053481899962                                                                                                                                                             \n",
      "Epoch 29/100, lr: 6.4e-04, average MSE (log10) loss: 0.0016467578851674892, average MAPE: 6.8904920422301                                                                                                                                                             \n",
      "Epoch 30/100, lr: 6.4e-04, average MSE (log10) loss: 0.0016214960568337415, average MAPE: 6.835737210877087                                                                                                                                                             \n",
      "Epoch 31/100, lr: 6.4e-04, average MSE (log10) loss: 0.0016009328920128091, average MAPE: 6.800268317242058                                                                                                                                                             \n",
      "Epoch 32/100, lr: 6.4e-04, average MSE (log10) loss: 0.001578568205313415, average MAPE: 6.751285428416972                                                                                                                                                             \n",
      "Epoch 33/100, lr: 6.4e-04, average MSE (log10) loss: 0.001558485337799149, average MAPE: 6.70878080445893                                                                                                                                                             \n",
      "Epoch 34/100, lr: 6.4e-04, average MSE (log10) loss: 0.0015393973275905058, average MAPE: 6.667572496375259                                                                                                                                                             \n",
      "Epoch 35/100, lr: 6.4e-04, average MSE (log10) loss: 0.0015164439243321515, average MAPE: 6.619801406471097                                                                                                                                                             \n",
      "Epoch 36/100, lr: 6.4e-04, average MSE (log10) loss: 0.0015010256614840151, average MAPE: 6.59041193748007                                                                                                                                                             \n",
      "Epoch 37/100, lr: 6.4e-04, average MSE (log10) loss: 0.0014832991688531272, average MAPE: 6.5543309581523035                                                                                                                                                             \n",
      "Epoch 38/100, lr: 6.4e-04, average MSE (log10) loss: 0.0014714325412309595, average MAPE: 6.530773676658163                                                                                                                                                             \n",
      "Epoch 39/100, lr: 6.4e-04, average MSE (log10) loss: 0.001454949244970874, average MAPE: 6.496753307264679                                                                                                                                                             \n",
      "Epoch 40/100, lr: 5.1e-04, average MSE (log10) loss: 0.0014293075277831177, average MAPE: 6.4363506862095425                                                                                                                                                             \n",
      "Epoch 41/100, lr: 5.1e-04, average MSE (log10) loss: 0.001412363354667869, average MAPE: 6.396549287134287                                                                                                                                                             \n",
      "Epoch 42/100, lr: 5.1e-04, average MSE (log10) loss: 0.0014076612642681112, average MAPE: 6.391567376195168                                                                                                                                                             \n",
      "Epoch 43/100, lr: 5.1e-04, average MSE (log10) loss: 0.0013879994966317804, average MAPE: 6.346183998730718                                                                                                                                                             \n",
      "Epoch 44/100, lr: 4.1e-04, average MSE (log10) loss: 0.0013750892678959941, average MAPE: 6.318814552073576                                                                                                                                                             \n",
      "Epoch 45/100, lr: 4.1e-04, average MSE (log10) loss: 0.0013637748746467488, average MAPE: 6.2945390078486225                                                                                                                                                             \n",
      "Epoch 46/100, lr: 4.1e-04, average MSE (log10) loss: 0.0013532680934485123, average MAPE: 6.268422866354183                                                                                                                                                             \n",
      "Epoch 47/100, lr: 4.1e-04, average MSE (log10) loss: 0.0013410401235961792, average MAPE: 6.244147185890042                                                                                                                                                             \n",
      "Epoch 48/100, lr: 4.1e-04, average MSE (log10) loss: 0.0013367105731550528, average MAPE: 6.235653976518281                                                                                                                                                             \n",
      "Epoch 49/100, lr: 4.1e-04, average MSE (log10) loss: 0.001327597669192723, average MAPE: 6.217872294601129                                                                                                                                                             \n",
      "Epoch 50/100, lr: 4.1e-04, average MSE (log10) loss: 0.0013160637332772721, average MAPE: 6.1902585710797995                                                                                                                                                             \n",
      "Epoch 51/100, lr: 4.1e-04, average MSE (log10) loss: 0.0013122468540559009, average MAPE: 6.185198682668258                                                                                                                                                             \n",
      "Epoch 52/100, lr: 3.3e-04, average MSE (log10) loss: 0.0012998441666630762, average MAPE: 6.155996462763572                                                                                                                                                             \n",
      "Epoch 53/100, lr: 3.3e-04, average MSE (log10) loss: 0.0012997452982188183, average MAPE: 6.157358893569635                                                                                                                                                             \n",
      "Epoch 54/100, lr: 3.3e-04, average MSE (log10) loss: 0.0012874172781879197, average MAPE: 6.1309556493953785                                                                                                                                                             \n",
      "Epoch 55/100, lr: 3.3e-04, average MSE (log10) loss: 0.0012794970629774795, average MAPE: 6.1100643177421725                                                                                                                                                             \n",
      "Epoch 56/100, lr: 2.6e-04, average MSE (log10) loss: 0.0012748522455899084, average MAPE: 6.100772810955437                                                                                                                                                             \n",
      "Epoch 57/100, lr: 2.6e-04, average MSE (log10) loss: 0.0012633626762663527, average MAPE: 6.073375598751769                                                                                                                                                             \n",
      "minibatch: 228/245, ratio: 1.4462e-02, best loss till now: 1.2595e-03, loss RMSE (log10) & MAPE -----  NN: 0.0012855538, 6.0513129; fit: 0.0888899565, 67.8757095\r"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "ratio_losses=[]\n",
    "mse_per_minibatch_nn=[]\n",
    "mape_per_minibatch_nn=[]\n",
    "mse_per_minibatch_fit=[]\n",
    "mape_per_minibatch_fit=[]\n",
    "for epoch in range(epochs):\n",
    "    total_mse = 0.0\n",
    "    total_mape = 0.0\n",
    "    for batch_idx,el in enumerate(dataloader):\n",
    "        minibatch=el[:,:-3].to(device)\n",
    "        altitude=el[:,-2].to(device)\n",
    "        rho_target=el[:,-1].to(device)\n",
    "        delta_params = model(minibatch).to(device)\n",
    "\n",
    "        #Constructs the inputs for the compute_approximated_density function as corrections from the global fit:\n",
    "        params = best_global_fit*(1+delta_params)\n",
    "        rho_nn=tn.rho_approximation(h=altitude,\n",
    "                                                params=params,\n",
    "                                                backend='torch')\n",
    "        rho_fit=tn.rho_approximation(h=altitude,\n",
    "                                             params=best_global_fit,\n",
    "                                             backend='torch')\n",
    "\n",
    "        loss = criterion(rho_nn, rho_target)\n",
    "\n",
    "        #Computes the global fit loss:\n",
    "        loss_fit =  criterion(rho_fit, rho_target)\n",
    "\n",
    "        # Zeroes the gradient (necessary because of things)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Perform a step in LR scheduler to update LR\n",
    "        scheduler.step(loss.item())\n",
    "\n",
    "        #We compute the logged quantities\n",
    "        mse_per_minibatch_nn.append(loss.item())\n",
    "        mape_per_minibatch_nn.append(tn.mean_absolute_percentage_error(rho_nn, rho_target).item())\n",
    "        total_mse+=mse_per_minibatch_nn[-1]\n",
    "        total_mape+=mape_per_minibatch_nn[-1]\n",
    "        \n",
    "        #Now the same but for the global fit:\n",
    "        mse_per_minibatch_fit.append(loss_fit.item())\n",
    "        mape_per_minibatch_fit.append(tn.mean_absolute_percentage_error(rho_fit, rho_target).item())\n",
    "\n",
    "        #Ratio of the loss between the NN and the fit (the lower, the more the NN is doing better than a global fit)\n",
    "        ratio_losses.append(loss.item()/loss_fit.item())\n",
    "        #Save the best model (this is wrong and should be done on the dataset):\n",
    "        if batch_idx>1:\n",
    "            if mse_per_minibatch_nn[-1]<min(mse_per_minibatch_nn[:-1]):    \n",
    "                #updating torch best model:\n",
    "                torch.save(model.state_dict(), f'../models/nrlmsise00_model_xxx.pyt')\n",
    "                best_loss=loss.item()\n",
    "                #print(f'Saving model - current best loss: {best_loss}\\n')\n",
    "        else:\n",
    "            best_loss=loss.item()\n",
    "        #Print every 10 minibatches:\n",
    "        if batch_idx%10:    \n",
    "            print(f'minibatch: {batch_idx}/{len(dataloader)}, ratio: {ratio_losses[-1]:.4e}, best loss till now: {best_loss:.4e}, loss RMSE (log10) & MAPE -----  NN: {loss.item():.10f}, {mape_per_minibatch_nn[-1]:.7f}; fit: {loss_fit.item():.10f}, {mape_per_minibatch_fit[-1]:.7f}', end='\\r')\n",
    "    #Print at the end of the epoch\n",
    "    curr_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, lr: {curr_lr:.1e}, average MSE (log10) loss: {total_mse / len(dataloader)}, average MAPE: {total_mape / len(dataloader)}                                                                                                                                                             ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_params = model(torch_data[:,:-3]).to(device)\n",
    "params = best_global_fit*(1+delta_params)\n",
    "rho_nn=tn.rho_approximation(h=torch_data[:,-2],params=params,backend='torch')\n",
    "rho_fit=tn.rho_approximation(h=torch_data[:,-2],params=best_global_fit,backend='torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.64500e+03, 2.64130e+04, 4.06470e+04, 4.90810e+04, 5.72110e+04,\n",
       "        7.10660e+04, 9.92010e+04, 1.13134e+05, 1.03492e+05, 8.40460e+04,\n",
       "        6.50050e+04, 4.75570e+04, 3.65600e+04, 2.81690e+04, 2.27910e+04,\n",
       "        1.85690e+04, 1.53900e+04, 1.29820e+04, 1.11430e+04, 9.65900e+03,\n",
       "        8.51100e+03, 7.53300e+03, 6.59300e+03, 5.89700e+03, 5.42100e+03,\n",
       "        4.88700e+03, 4.48700e+03, 4.00200e+03, 3.50700e+03, 3.02000e+03,\n",
       "        2.87500e+03, 2.59500e+03, 2.19800e+03, 1.99900e+03, 1.73000e+03,\n",
       "        1.54300e+03, 1.40000e+03, 1.22100e+03, 1.07700e+03, 9.33000e+02,\n",
       "        8.67000e+02, 7.99000e+02, 7.22000e+02, 6.26000e+02, 5.09000e+02,\n",
       "        4.84000e+02, 4.68000e+02, 4.05000e+02, 3.59000e+02, 3.49000e+02,\n",
       "        2.93000e+02, 2.94000e+02, 2.83000e+02, 2.28000e+02, 2.00000e+02,\n",
       "        1.67000e+02, 1.66000e+02, 1.41000e+02, 1.45000e+02, 1.29000e+02,\n",
       "        1.14000e+02, 9.50000e+01, 8.10000e+01, 7.80000e+01, 7.30000e+01,\n",
       "        6.00000e+01, 5.90000e+01, 7.40000e+01, 4.90000e+01, 4.20000e+01,\n",
       "        3.00000e+01, 2.40000e+01, 2.40000e+01, 1.10000e+01, 2.30000e+01,\n",
       "        9.00000e+00, 1.60000e+01, 7.00000e+00, 1.50000e+01, 1.40000e+01,\n",
       "        1.00000e+01, 6.00000e+00, 6.00000e+00, 7.00000e+00, 7.00000e+00,\n",
       "        1.00000e+00, 4.00000e+00, 3.00000e+00, 5.00000e+00, 4.00000e+00,\n",
       "        2.00000e+00, 0.00000e+00, 2.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        3.00000e+00, 2.00000e+00, 7.00000e+00, 2.00000e+00, 5.00000e+00]),\n",
       " array([ 0.03778633,  0.15152147,  0.26525661,  0.37899174,  0.49272688,\n",
       "         0.60646202,  0.72019715,  0.83393229,  0.94766743,  1.06140256,\n",
       "         1.1751377 ,  1.28887284,  1.40260797,  1.51634311,  1.63007825,\n",
       "         1.74381338,  1.85754852,  1.97128366,  2.08501879,  2.19875393,\n",
       "         2.31248907,  2.4262242 ,  2.53995934,  2.65369448,  2.76742961,\n",
       "         2.88116475,  2.99489989,  3.10863502,  3.22237016,  3.3361053 ,\n",
       "         3.44984043,  3.56357557,  3.67731071,  3.79104584,  3.90478098,\n",
       "         4.01851612,  4.13225126,  4.24598639,  4.35972153,  4.47345667,\n",
       "         4.5871918 ,  4.70092694,  4.81466208,  4.92839721,  5.04213235,\n",
       "         5.15586749,  5.26960262,  5.38333776,  5.4970729 ,  5.61080803,\n",
       "         5.72454317,  5.83827831,  5.95201344,  6.06574858,  6.17948372,\n",
       "         6.29321885,  6.40695399,  6.52068913,  6.63442426,  6.7481594 ,\n",
       "         6.86189454,  6.97562967,  7.08936481,  7.20309995,  7.31683508,\n",
       "         7.43057022,  7.54430536,  7.65804049,  7.77177563,  7.88551077,\n",
       "         7.99924591,  8.11298104,  8.22671618,  8.34045132,  8.45418645,\n",
       "         8.56792159,  8.68165673,  8.79539186,  8.909127  ,  9.02286214,\n",
       "         9.13659727,  9.25033241,  9.36406755,  9.47780268,  9.59153782,\n",
       "         9.70527296,  9.81900809,  9.93274323, 10.04647837, 10.1602135 ,\n",
       "        10.27394864, 10.38768378, 10.50141891, 10.61515405, 10.72888919,\n",
       "        10.84262432, 10.95635946, 11.0700946 , 11.18382973, 11.29756487,\n",
       "        11.41130001]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = rho_nn.detach().numpy()\n",
    "gt = target_density\n",
    "plt.hist((predictions/gt),bins=100,alpha=0.5)\n",
    "plt.hist((rho_fit/gt),bins=100,alpha=0.5)\n",
    "\n",
    "#plt.hist(np.abs(predictions-gt)/np.abs(gt)*100, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermonets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
