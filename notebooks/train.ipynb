{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import thermonets\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of database is: (999800, 16)\n"
     ]
    }
   ],
   "source": [
    "#I load the data generated via `/scripts/generate_nrlmsise00_db.py` and print the columns\n",
    "#note that columns can be (len 16):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'wind zonal [m/s]', 'wind meridional [m/s]', 'density [kg/m^3]'\n",
    "#or (len 14):\n",
    "#'day', 'month', 'year', 'hour', 'minute', 'second', 'microsecond', 'alt [km]', 'lat [deg]', 'lon [deg]', 'f107A', 'f107', 'ap', 'density [kg/m^3]'\n",
    "db=np.loadtxt('../dbs/nrlmsise00_db.txt',delimiter=',',skiprows=1)\n",
    "print(f'Shape of database is: {db.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds in day min and max:\n",
      "86381.805516 5.198408\n",
      "day of the year min and max:\n",
      "365.0 1.0\n"
     ]
    }
   ],
   "source": [
    "#I now construct the day of the year and seconds in day:\n",
    "years=db[:,2]\n",
    "months=db[:,1]\n",
    "days=db[:,0]\n",
    "hours=db[:,3]\n",
    "minutes=db[:,4]\n",
    "seconds=db[:,5]\n",
    "microseconds=db[:,6]\n",
    "seconds_in_day=hours*3600+minutes*60+seconds+microseconds/1e6\n",
    "print('seconds in day min and max:')\n",
    "print(seconds_in_day.max(), seconds_in_day.min())\n",
    "doys=np.zeros(db.shape[0])\n",
    "for i in range(len(db)):\n",
    "    #date is a string, so I first convert it to datetime:\n",
    "    date_=datetime.datetime(year=int(years[i]), \n",
    "                            month=int(months[i]), \n",
    "                            day=int(days[i]),\n",
    "                            hour=int(hours[i]),\n",
    "                            minute=int(minutes[i]),\n",
    "                            second=int(seconds[i]),\n",
    "                            microsecond=int(microseconds[i]))\n",
    "    doys[i]=date_.timetuple().tm_yday\n",
    "print('day of the year min and max:')\n",
    "print(doys.max(), doys.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I extract the altitude:\n",
    "alt=db[:,7]\n",
    "#I now extract the longitude and latitude, and convert them to radians:\n",
    "lat=np.deg2rad(db[:,8])\n",
    "lon=np.deg2rad(db[:,9])\n",
    "#now the space weather indices:\n",
    "f107a=db[:,10]\n",
    "f107=db[:,11]\n",
    "ap=db[:,12]\n",
    "#let's extract the target density as well:\n",
    "target_density=db[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function normalizes the data to the range [-1,1]\n",
    "def normalize_min_max(data,min_val,max_val):\n",
    "    normalized_data = (2 * (data - min_val) / (max_val - min_val)) - 1\n",
    "    return normalized_data\n",
    "def unnormalize_min_max(data,min_val,max_val):\n",
    "    unnormalized_data = 1/2 * (data + 1) * (max_val - min_val) + min_val\n",
    "    return unnormalized_data\n",
    "#verify: unnormalize_min_max(normalize_min_max(alt,alt.min(),alt.max()),alt.min(),alt.max())==alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_normalized=np.zeros((db.shape[0],13))\n",
    "db_normalized[:,0]=np.sin(lon)\n",
    "db_normalized[:,1]=np.cos(lon)\n",
    "db_normalized[:,2]=np.sin(lat)\n",
    "db_normalized[:,3]=np.sin(2*np.pi*seconds_in_day/86400.)\n",
    "db_normalized[:,4]=np.cos(2*np.pi*seconds_in_day/86400.)\n",
    "db_normalized[:,5]=np.sin(2*np.pi*doys/365.25)\n",
    "db_normalized[:,6]=np.cos(2*np.pi*doys/365.25)\n",
    "db_normalized[:,7]=normalize_min_max(alt, 150., 650.)\n",
    "db_normalized[:,8]=normalize_min_max(f107, 60., 290.)\n",
    "db_normalized[:,9]=normalize_min_max(f107a, 50., 190.)\n",
    "db_normalized[:,10]=normalize_min_max(ap, 0., 140.)\n",
    "#I add the non-normalized density & altitude columns (useful to extract during training):\n",
    "db_normalized[:,11]=alt\n",
    "db_normalized[:,12] = target_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum and minimum of all the normalized data: 630.957344480193, -1.0\n",
      "maximum and minimum of target density: 1.720909357141654e-09, 2.1621841115988526e-15\n"
     ]
    }
   ],
   "source": [
    "#cross check that the max is <=1 and the min is >=-1\n",
    "print(f\"maximum and minimum of all the normalized data: {db_normalized.max()}, {db_normalized.min()}\")\n",
    "print(f\"maximum and minimum of target density: {target_density.max()}, {target_density.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = torch.tensor(db_normalized,\n",
    "                          dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN hyperparameters\n",
    "device = torch.device('cpu')\n",
    "batch_size = 4096\n",
    "model_path = None #pass a path to a model in case you want to continue training\n",
    "lr = 0.00001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader creation\n",
    "dataloader = torch.utils.data.DataLoader(torch_data, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN creation\n",
    "model = thermonets.ffnn(input_dim=db_normalized.shape[1]-2,\n",
    "                        hidden_layer_dims=[32, 32],\n",
    "                        output_dim=12,\n",
    "                        mid_activation=torch.nn.Tanh(),\n",
    "                        last_activation=torch.nn.Tanh()).to(device)\n",
    "\n",
    "if model_path is not None:\n",
    "    model.load_state_dict(torch.load(model_path,\n",
    "                                     map_location=device.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters: 1836\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of model parameters: {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the global fit (see notebook: `rho_global_fit.ipynb`: this will be the baseline from which we ask the NN to learn corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ga00693/Develop/thermonets/global_fits/global_fit_nrlmsise00_180.0-1000.0-4.txt','rb') as f:\n",
    "    best_global_fit=torch.from_numpy(pickle.load(f)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute the mean absolute percentage error (MAPE) between true and predicted values.\n",
    "    \n",
    "    Args:\n",
    "        y_true (`torch.tensor`): True values.\n",
    "        y_pred (`torch.tensor`): Predicted values.\n",
    "        \n",
    "    Returns:\n",
    "        `torch.tensor`: Mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1/100, average RMSE (log10) loss: 0.1348125112604122, average MAPE: 72.86179965272241, -----  NN: 0.1247022524, 62.6941757; fit: 0.0751954949, 48.6998353\n",
      "End of epoch 2/100, average RMSE (log10) loss: 0.10745000820987079, average MAPE: 63.015891826396086, ---  NN: 0.0902358145, 58.7182884; fit: 0.0723802327, 49.0179714\n",
      "End of epoch 3/100, average RMSE (log10) loss: 0.08764881166268368, average MAPE: 56.7876057994609, -----  NN: 0.0800742283, 49.7547569; fit: 0.0771722338, 46.0925415\n",
      "End of epoch 4/100, average RMSE (log10) loss: 0.07243711650371551, average MAPE: 51.60860265615035, ----  NN: 0.0787589476, 57.5506401; fit: 0.0879144889, 60.0736066\n",
      "End of epoch 5/100, average RMSE (log10) loss: 0.060235488125864343, average MAPE: 46.72686671042929, ---  NN: 0.0522562303, 47.3934593; fit: 0.0651796062, 52.0998396\n",
      "End of epoch 6/100, average RMSE (log10) loss: 0.05052070438253636, average MAPE: 42.270780913683836, ---  NN: 0.0589060634, 45.0034676; fit: 0.0983349249, 56.7957620\n",
      "End of epoch 7/100, average RMSE (log10) loss: 0.042708311260354764, average MAPE: 38.427068126444915, --  NN: 0.0392538682, 36.8816528; fit: 0.0783518792, 52.1265252\n",
      "End of epoch 8/100, average RMSE (log10) loss: 0.03677531752963455, average MAPE: 35.17827769688198, ----  NN: 0.0336628258, 33.1808205; fit: 0.0729736347, 48.9089754\n",
      "End of epoch 9/100, average RMSE (log10) loss: 0.03247827881452989, average MAPE: 32.74507752243353, ----  NN: 0.0306368135, 33.6147423; fit: 0.0848630472, 55.6589258\n",
      "End of epoch 10/100, average RMSE (log10) loss: 0.02953875606157342, average MAPE: 31.03942721619898, ---  NN: 0.0256793015, 29.2838326; fit: 0.0724035140, 48.7289846\n",
      "End of epoch 11/100, average RMSE (log10) loss: 0.027637462622048903, average MAPE: 29.9152614282102, ---  NN: 0.0227563437, 27.0749607; fit: 0.0671023823, 45.6300766\n",
      "End of epoch 12/100, average RMSE (log10) loss: 0.026433143179331507, average MAPE: 29.200944853802117, -  NN: 0.0256309528, 27.3664207; fit: 0.0798104890, 51.0478484\n",
      "End of epoch 13/100, average RMSE (log10) loss: 0.025591480914427308, average MAPE: 28.705841375856984, -  NN: 0.0243046656, 28.2521305; fit: 0.0868956564, 52.5491991\n",
      "End of epoch 14/100, average RMSE (log10) loss: 0.024948572625919263, average MAPE: 28.32188465351961, --  NN: 0.0267729349, 29.4933281; fit: 0.0774009473, 55.8817377\n",
      "End of epoch 15/100, average RMSE (log10) loss: 0.024371554079104442, average MAPE: 27.957644629965024, -  NN: 0.0257948507, 28.6687584; fit: 0.0729417642, 49.3292417\n",
      "End of epoch 16/100, average RMSE (log10) loss: 0.023811682427720148, average MAPE: 27.622086886970365, -  NN: 0.0213868413, 24.8226910; fit: 0.0697713966, 46.4679671\n",
      "End of epoch 17/100, average RMSE (log10) loss: 0.02329042168448166, average MAPE: 27.31224253128986, ---  NN: 0.0245071873, 28.4463921; fit: 0.0740141278, 49.5198644\n",
      "End of epoch 18/100, average RMSE (log10) loss: 0.022761470049011464, average MAPE: 26.966078365092375, -  NN: 0.0249563716, 27.9706841; fit: 0.0709454425, 49.1072931\n",
      "End of epoch 19/100, average RMSE (log10) loss: 0.022214313064302715, average MAPE: 26.626817648751395, -  NN: 0.0204638261, 24.4067764; fit: 0.0753855006, 47.4105109\n",
      "End of epoch 20/100, average RMSE (log10) loss: 0.021693325939835334, average MAPE: 26.30255734190649, --  NN: 0.0232236870, 27.4134483; fit: 0.0750639902, 53.1456581\n",
      "End of epoch 21/100, average RMSE (log10) loss: 0.021153216862252782, average MAPE: 25.948298722870497, -  NN: 0.0214176457, 27.0974579; fit: 0.0900565056, 53.8021649\n",
      "End of epoch 22/100, average RMSE (log10) loss: 0.020611682366959903, average MAPE: 25.57357696221799, --  NN: 0.0195145644, 23.6608658; fit: 0.0810982342, 48.7066325\n",
      "End of epoch 23/100, average RMSE (log10) loss: 0.020070226277623858, average MAPE: 25.227773775373187, -  NN: 0.0185436066, 24.6647320; fit: 0.0672952025, 48.3554190\n",
      "End of epoch 24/100, average RMSE (log10) loss: 0.01952591912478817, average MAPE: 24.852915939019653, --  NN: 0.0171608739, 23.4261131; fit: 0.0731117508, 49.1947998\n",
      "End of epoch 25/100, average RMSE (log10) loss: 0.018990538421334052, average MAPE: 24.49056773283044, --  NN: 0.0192478672, 26.3865509; fit: 0.0732561270, 56.4159649\n",
      "End of epoch 26/100, average RMSE (log10) loss: 0.018439221655835912, average MAPE: 24.094595111146266, -  NN: 0.0174529254, 22.4793606; fit: 0.0605235848, 43.2792566\n",
      "End of epoch 27/100, average RMSE (log10) loss: 0.01789416495178427, average MAPE: 23.720446123395647, --  NN: 0.0171767455, 23.8874512; fit: 0.0860851751, 54.1789357\n",
      "End of epoch 28/100, average RMSE (log10) loss: 0.017360713310083563, average MAPE: 23.324138742563676, -  NN: 0.0195851550, 24.1656475; fit: 0.0855023738, 54.8044612\n",
      "End of epoch 29/100, average RMSE (log10) loss: 0.016807039249308254, average MAPE: 22.923072246629363, -  NN: 0.0166459773, 23.2175446; fit: 0.0705564785, 50.3580730\n",
      "End of epoch 30/100, average RMSE (log10) loss: 0.016269284851697027, average MAPE: 22.532985609404896, -  NN: 0.0179374348, 23.5565624; fit: 0.0727846004, 51.9716359\n",
      "End of epoch 31/100, average RMSE (log10) loss: 0.015724202891697688, average MAPE: 22.115276367810306, -  NN: 0.0164289325, 22.0121384; fit: 0.0732641181, 49.5760859\n",
      "End of epoch 32/100, average RMSE (log10) loss: 0.015184464590738014, average MAPE: 21.71469361441476, --  NN: 0.0157245882, 21.8508892; fit: 0.0835136493, 51.1301743\n",
      "End of epoch 33/100, average RMSE (log10) loss: 0.014647694905193484, average MAPE: 21.293443664239376, -  NN: 0.0138843693, 19.9442253; fit: 0.0728503534, 49.5186377\n",
      "End of epoch 34/100, average RMSE (log10) loss: 0.014129535435717933, average MAPE: 20.89800893043985, --  NN: 0.0144973900, 20.9678364; fit: 0.0766730112, 51.7113376\n",
      "End of epoch 35/100, average RMSE (log10) loss: 0.01361897750472536, average MAPE: 20.503399946251694, --  NN: 0.0141325556, 21.1621857; fit: 0.0782506987, 51.9604731\n",
      "End of epoch 36/100, average RMSE (log10) loss: 0.01311622066233231, average MAPE: 20.104184870817225, --  NN: 0.0132339559, 20.5124111; fit: 0.0862430434, 56.0421432\n",
      "End of epoch 37/100, average RMSE (log10) loss: 0.01263043472581372, average MAPE: 19.71306491773956, ---  NN: 0.0133764930, 20.2523251; fit: 0.0785173100, 51.9120264\n",
      "End of epoch 38/100, average RMSE (log10) loss: 0.012150868203262893, average MAPE: 19.331354374788244, -  NN: 0.0120476447, 19.3769970; fit: 0.0721632378, 47.8581933\n",
      "End of epoch 39/100, average RMSE (log10) loss: 0.011684351102734098, average MAPE: 18.95406929327517, --  NN: 0.0119523825, 19.6000900; fit: 0.0895538067, 54.6235573\n",
      "End of epoch 40/100, average RMSE (log10) loss: 0.01122330509278239, average MAPE: 18.581162145186443, --  NN: 0.0122977737, 19.7485237; fit: 0.0786307212, 49.6724178\n",
      "End of epoch 41/100, average RMSE (log10) loss: 0.010752549417773071, average MAPE: 18.19641814718441, --  NN: 0.0116835693, 19.3473835; fit: 0.0971334811, 56.7088228\n",
      "End of epoch 42/100, average RMSE (log10) loss: 0.010261237990035086, average MAPE: 17.810403481308295, -  NN: 0.0110468827, 19.4135094; fit: 0.0750535356, 55.2043581\n",
      "End of epoch 43/100, average RMSE (log10) loss: 0.009764429634170874, average MAPE: 17.404761855456293, -  NN: 0.0107470620, 18.1307468; fit: 0.0854631559, 53.0523525\n",
      "End of epoch 44/100, average RMSE (log10) loss: 0.009299133391100533, average MAPE: 17.00845120488381, --  NN: 0.0083118752, 15.6769123; fit: 0.0770285725, 45.9173956\n",
      "End of epoch 45/100, average RMSE (log10) loss: 0.00891719223095142, average MAPE: 16.655407726521396, --  NN: 0.0077086394, 15.6179609; fit: 0.0775178700, 49.2056032\n",
      "End of epoch 46/100, average RMSE (log10) loss: 0.008607426103280516, average MAPE: 16.351332668382295, -  NN: 0.0085433694, 16.8658218; fit: 0.0781008671, 57.3642816\n",
      "End of epoch 47/100, average RMSE (log10) loss: 0.008337342689688108, average MAPE: 16.064501217433385, -  NN: 0.0080111371, 15.3648224; fit: 0.0696051085, 42.9757584\n",
      "End of epoch 48/100, average RMSE (log10) loss: 0.00809746968502901, average MAPE: 15.820244057324468, --  NN: 0.0083198855, 16.0852890; fit: 0.0758552734, 54.1392671\n",
      "End of epoch 49/100, average RMSE (log10) loss: 0.007873844507397438, average MAPE: 15.577421892905722, -  NN: 0.0079933014, 15.0800095; fit: 0.0720798141, 48.6092880\n",
      "End of epoch 50/100, average RMSE (log10) loss: 0.007658488088648538, average MAPE: 15.347993360246932, -  NN: 0.0065625333, 14.3072233; fit: 0.0772026294, 49.1054958\n",
      "End of epoch 51/100, average RMSE (log10) loss: 0.007456979216361533, average MAPE: 15.125521788305166, -  NN: 0.0059093852, 13.4903297; fit: 0.0640223197, 44.5271516\n",
      "End of epoch 52/100, average RMSE (log10) loss: 0.007268391055416087, average MAPE: 14.917147644198671, -  NN: 0.0060969545, 13.8006325; fit: 0.0736295350, 50.8678040\n",
      "End of epoch 53/100, average RMSE (log10) loss: 0.007085480252090766, average MAPE: 14.708367199800453, -  NN: 0.0052076303, 12.7561388; fit: 0.0640184989, 42.1098644\n",
      "End of epoch 54/100, average RMSE (log10) loss: 0.006920045502103713, average MAPE: 14.514938117046745, -  NN: 0.0068361764, 14.4238873; fit: 0.0772644438, 49.4038448\n",
      "End of epoch 55/100, average RMSE (log10) loss: 0.006755811816119418, average MAPE: 14.324260279597068, -  NN: 0.0064337160, 13.8146906; fit: 0.0934685360, 54.9747542\n",
      "End of epoch 56/100, average RMSE (log10) loss: 0.006603099538811615, average MAPE: 14.139263818701918, -  NN: 0.0069777584, 14.2669783; fit: 0.0756452701, 50.0147106\n",
      "End of epoch 57/100, average RMSE (log10) loss: 0.0064516792530003855, average MAPE: 13.953866946940519,   NN: 0.0057846499, 12.6461143; fit: 0.0740726228, 44.6964084\n",
      "End of epoch 58/100, average RMSE (log10) loss: 0.006312611611674027, average MAPE: 13.785066982191436, -  NN: 0.0059539201, 13.5477457; fit: 0.0926536122, 55.7834053\n",
      "End of epoch 59/100, average RMSE (log10) loss: 0.006178266861076866, average MAPE: 13.614768257919623, -  NN: 0.0054302607, 13.3370991; fit: 0.0798573750, 55.3939586\n",
      "End of epoch 60/100, average RMSE (log10) loss: 0.00605621462003613, average MAPE: 13.455764898961904, --  NN: 0.0061567975, 13.6303339; fit: 0.0791206012, 51.7446083\n",
      "End of epoch 61/100, average RMSE (log10) loss: 0.005933318465796052, average MAPE: 13.295395208864797, -  NN: 0.0049982285, 12.4937859; fit: 0.0638664912, 48.0228444\n",
      "End of epoch 62/100, average RMSE (log10) loss: 0.005824599373249375, average MAPE: 13.147076704064194, -  NN: 0.0057104165, 12.9283752; fit: 0.0783792113, 54.2611604\n",
      "End of epoch 63/100, average RMSE (log10) loss: 0.0057190724990653745, average MAPE: 13.006254624347298,   NN: 0.0055820281, 13.0780878; fit: 0.0847265695, 54.6204166\n",
      "End of epoch 64/100, average RMSE (log10) loss: 0.005617021989761567, average MAPE: 12.86144465232382, --  NN: 0.0046553109, 11.5092945; fit: 0.0671555660, 47.3749102\n",
      "End of epoch 65/100, average RMSE (log10) loss: 0.005525595449595427, average MAPE: 12.736174676856216, -  NN: 0.0047976421, 12.0090971; fit: 0.0819257579, 51.4298153\n",
      "End of epoch 66/100, average RMSE (log10) loss: 0.005439605176144717, average MAPE: 12.613193153848453, -  NN: 0.0051124175, 12.3803034; fit: 0.0693908156, 45.0768098\n",
      "End of epoch 67/100, average RMSE (log10) loss: 0.00536216941605113, average MAPE: 12.4962999499574, ----  NN: 0.0062254420, 12.6527395; fit: 0.0694018482, 51.9545172\n",
      "End of epoch 68/100, average RMSE (log10) loss: 0.005282489596200841, average MAPE: 12.38136274765949, --  NN: 0.0053184927, 11.7127428; fit: 0.0675929671, 47.2684728\n",
      "End of epoch 69/100, average RMSE (log10) loss: 0.00521607406209318, average MAPE: 12.282456144994619, --  NN: 0.0068558110, 13.8610125; fit: 0.0966378629, 60.8764968\n",
      "End of epoch 70/100, average RMSE (log10) loss: 0.005141005212707179, average MAPE: 12.16995299981565, --  NN: 0.0048216060, 11.7288837; fit: 0.0719390413, 51.0734768\n",
      "End of epoch 71/100, average RMSE (log10) loss: 0.005076366233430347, average MAPE: 12.073613598881936, -  NN: 0.0045410702, 11.4360447; fit: 0.0796765839, 50.6092830\n",
      "End of epoch 72/100, average RMSE (log10) loss: 0.005020850166982534, average MAPE: 11.987796849620585, -  NN: 0.0057150582, 12.6315269; fit: 0.0722544179, 51.7034143\n",
      "End of epoch 73/100, average RMSE (log10) loss: 0.004963053711594976, average MAPE: 11.899156231782873, -  NN: 0.0052740769, 12.4513235; fit: 0.0735658279, 50.8114096\n",
      "End of epoch 74/100, average RMSE (log10) loss: 0.004906638650869837, average MAPE: 11.811803331180494, -  NN: 0.0045793736, 11.5659857; fit: 0.0861737166, 51.3949799\n",
      "End of epoch 75/100, average RMSE (log10) loss: 0.004855745521431067, average MAPE: 11.737530393016582, -  NN: 0.0045775864, 12.1725607; fit: 0.0783212783, 52.5624383\n",
      "End of epoch 76/100, average RMSE (log10) loss: 0.0048064723287766075, average MAPE: 11.66033886890022, -  NN: 0.0044798031, 11.8104000; fit: 0.0747784776, 50.6746135\n",
      "End of epoch 77/100, average RMSE (log10) loss: 0.004758494632432655, average MAPE: 11.582859074339575, -  NN: 0.0040704515, 10.9610891; fit: 0.0703262092, 46.3203546\n",
      "End of epoch 78/100, average RMSE (log10) loss: 0.004720319554741894, average MAPE: 11.515746330728335, -  NN: 0.0058421348, 11.8567972; fit: 0.0791401053, 55.4198836\n",
      "End of epoch 79/100, average RMSE (log10) loss: 0.004675067524064561, average MAPE: 11.452429584581024, -  NN: 0.0052257227, 11.7511549; fit: 0.0824093944, 53.8720391\n",
      "End of epoch 80/100, average RMSE (log10) loss: 0.004631627265515984, average MAPE: 11.383565011316417, -  NN: 0.0046491101, 11.5801525; fit: 0.0805018336, 48.5398041\n",
      "End of epoch 81/100, average RMSE (log10) loss: 0.004590708884049435, average MAPE: 11.319313333472428, -  NN: 0.0043887994, 11.0640745; fit: 0.0653376563, 50.7198833\n",
      "End of epoch 82/100, average RMSE (log10) loss: 0.004551420476743761, average MAPE: 11.258049704104053, -  NN: 0.0042181029, 10.5383959; fit: 0.0788539437, 53.2093008\n",
      "End of epoch 83/100, average RMSE (log10) loss: 0.004515501965141418, average MAPE: 11.199087357034488, -  NN: 0.0046999478, 10.5414000; fit: 0.0875621139, 58.3800799\n",
      "End of epoch 84/100, average RMSE (log10) loss: 0.004479340511393182, average MAPE: 11.145389806007852, -  NN: 0.0047659609, 11.4783783; fit: 0.0821133069, 53.6329588\n",
      "End of epoch 85/100, average RMSE (log10) loss: 0.004440949344057209, average MAPE: 11.086690314935177, -  NN: 0.0040721251, 10.7131672; fit: 0.0676509393, 47.0928241\n",
      "End of epoch 86/100, average RMSE (log10) loss: 0.004404060269838997, average MAPE: 11.031153484266632, -  NN: 0.0035410582, 10.2252598; fit: 0.0783175560, 49.9936484\n",
      "End of epoch 87/100, average RMSE (log10) loss: 0.00437358996583795, average MAPE: 10.981107229116011, --  NN: 0.0044536465, 11.5076027; fit: 0.0743698322, 57.4279411\n",
      "End of epoch 88/100, average RMSE (log10) loss: 0.004338194555317869, average MAPE: 10.926996075377172, -  NN: 0.0039949897, 10.8917551; fit: 0.0762715251, 58.8328439\n",
      "End of epoch 89/100, average RMSE (log10) loss: 0.0043095340999793645, average MAPE: 10.879734669899454,   NN: 0.0050507779, 11.7702398; fit: 0.0832243000, 52.6678756\n",
      "End of epoch 90/100, average RMSE (log10) loss: 0.0042766161852193126, average MAPE: 10.826537864062251,   NN: 0.0048582121, 10.8047562; fit: 0.0888035693, 51.4865743\n",
      "End of epoch 91/100, average RMSE (log10) loss: 0.004244696162641048, average MAPE: 10.775121758908641, -  NN: 0.0045936559, 10.4585314; fit: 0.0798561603, 51.8766573\n",
      "End of epoch 92/100, average RMSE (log10) loss: 0.004213505893071391, average MAPE: 10.730648943842674, -  NN: 0.0043713348, 11.0891380; fit: 0.0760450759, 56.2439967\n",
      "End of epoch 93/100, average RMSE (log10) loss: 0.004181350227825496, average MAPE: 10.681988225664412, -  NN: 0.0036371020, 10.3946934; fit: 0.0782913946, 48.5782997\n",
      "End of epoch 94/100, average RMSE (log10) loss: 0.004160979484226935, average MAPE: 10.639580049320143, -  NN: 0.0059869322, 11.8148518; fit: 0.0767879266, 51.2384024\n",
      "End of epoch 95/100, average RMSE (log10) loss: 0.004123294195730467, average MAPE: 10.591663485157246, -  NN: 0.0034716038, 10.3389111; fit: 0.0869630556, 51.9857856\n",
      "End of epoch 96/100, average RMSE (log10) loss: 0.004097296566493353, average MAPE: 10.54864239984629, --  NN: 0.0037984936, 10.6818304; fit: 0.0740055208, 50.9053706\n",
      "End of epoch 97/100, average RMSE (log10) loss: 0.004070374046509363, average MAPE: 10.507077014689543, -  NN: 0.0038121166, 10.4830294; fit: 0.0765503040, 49.9129378\n",
      "End of epoch 98/100, average RMSE (log10) loss: 0.004048843499349088, average MAPE: 10.464448450049575, -  NN: 0.0051756892, 10.7324162; fit: 0.0622632709, 47.9948977\n",
      "End of epoch 99/100, average RMSE (log10) loss: 0.0040153603687198185, average MAPE: 10.423648861476353,   NN: 0.0030373281, 9.6877975; fit: 0.0699453208, 53.81000434\n",
      "End of epoch 100/100, average RMSE (log10) loss: 0.0039936645412627534, average MAPE: 10.385752179671307,  NN: 0.0039743185, 10.5490322; fit: 0.0709388183, 46.9825918\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "ratio_losses=[]\n",
    "rmse_per_minibatch_nn=[]\n",
    "mape_per_minibatch_nn=[]\n",
    "rmse_per_minibatch_fit=[]\n",
    "mape_per_minibatch_fit=[]\n",
    "for epoch in range(epochs):\n",
    "    model.train(True)  # Set model to training mode\n",
    "    total_rmse = 0.0\n",
    "    total_mape = 0.0\n",
    "    k=0\n",
    "    for batch_idx,el in enumerate(dataloader):\n",
    "        minibatch=el[:,:-2]\n",
    "        altitude=el[:,-2]\n",
    "        optimizer.zero_grad()  # Clear accumulated gradients    \n",
    "        minibatch=minibatch.to(device)\n",
    "        params = model(minibatch).to(device)\n",
    "\n",
    "        k+=1\n",
    "\n",
    "        minibatch=minibatch.to(device)\n",
    "        optimizer.zero_grad()  # Clear accumulated gradients\n",
    "        delta_params = model(minibatch).to(device)\n",
    "        #now I construct the inputs for the compute_approximated_density function as corrections from the global fit:\n",
    "        params = best_global_fit*(1+delta_params)\n",
    "        rho_nn=thermonets.rho_approximation(h=altitude,\n",
    "                                                params=params,\n",
    "                                                backend='torch')\n",
    "        rho_fit=torch.from_numpy(thermonets.rho_approximation(h=altitude.numpy(),\n",
    "                                                                params=best_global_fit.numpy()))\n",
    "        rho_target=el[:,-1].to(device)\n",
    "\n",
    "        loss = criterion(torch.log10(rho_nn), torch.log10(rho_target))\n",
    "        loss.backward()\n",
    "        #I also compute the global fit loss:\n",
    "        loss_fit =  torch.nn.MSELoss()(torch.log10(rho_fit).squeeze(), torch.log10(rho_target).squeeze())\n",
    "        #I update the weights:\n",
    "        optimizer.step()\n",
    "        #let's store the losses for the NN:\n",
    "        rmse_per_minibatch_nn.append(loss.item())\n",
    "        mape_per_minibatch_nn.append(mean_absolute_percentage_error(rho_nn, rho_target).item())\n",
    "        total_rmse+=rmse_per_minibatch_nn[-1]\n",
    "        total_mape+=mape_per_minibatch_nn[-1]\n",
    "        #now the same but for the global fit:\n",
    "        rmse_per_minibatch_fit.append(loss_fit.item())\n",
    "        mape_per_minibatch_fit.append(mean_absolute_percentage_error(rho_fit, rho_target).item())\n",
    "\n",
    "        #ratio of the loss between the NN and the fit (the lower, the more the NN is doing better than a global fit)\n",
    "        ratio_losses.append(loss.item()/loss_fit.item())\n",
    "        #I only save the best model:\n",
    "        if k>1:\n",
    "            if rmse_per_minibatch_nn[-1]<min(rmse_per_minibatch_nn[:-1]):    \n",
    "                #updating torch best model:\n",
    "                torch.save(model.state_dict(), f'best_model.pth')\n",
    "                best_loss=loss.item()\n",
    "                #print(f'Saving model - current best loss: {best_loss}\\n')\n",
    "        else:\n",
    "            best_loss=loss.item()\n",
    "        #I print every 10 minibatches:\n",
    "        if k%10:    \n",
    "            print(f'minibatch: {k}/{len(dataloader)}, ratio: {ratio_losses[-1]:.10f}, best loss till now: {best_loss:.10f}, loss RMSE (log10) & MAPE -----  NN: {loss.item():.10f}, {mape_per_minibatch_nn[-1]:.7f}; fit: {loss_fit.item():.10f}, {mape_per_minibatch_fit[-1]:.7f}', end='\\r')\n",
    "    #I also print at the end of the epoch\n",
    "    print(f'End of epoch {epoch + 1}/{epochs}, average RMSE (log10) loss: {total_rmse / len(dataloader)}, average MAPE: {total_mape / len(dataloader)}, ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermonets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
